@article{clarkCANINEPretrainingEfficient2022a,
  title = {{{CANINE}}: {{Pre-training}} an {{Efficient Tokenization-Free Encoder}} for {{Language Representation}}},
  shorttitle = {{{CANINE}}},
  author = {Clark, Jonathan H. and Garrette, Dan and Turc, Iulia and Wieting, John},
  year = {2022},
  month = jan,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  eprint = {2103.06874},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {73--91},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00448},
  abstract = {Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly-used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model's ability to adapt. In this paper, we present CANINE, a neural encoder that operates directly on character sequences, without explicit tokenization or vocabulary, and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. CANINE outperforms a comparable mBERT model by 2.8 F1 on TyDi QA, a challenging multilingual benchmark, despite having 28\% fewer model parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/patrickhaller/Zotero/storage/8ALHI42R/Clark et al. - 2022 - CANINE Pre-training an Efficient Tokenization-Fre.pdf;/Users/patrickhaller/Zotero/storage/BFKGK9M9/2103.html}
}

@misc{svenstrupHashEmbeddingsEfficient2017,
  title = {Hash {{Embeddings}} for {{Efficient Word Representations}}},
  author = {Svenstrup, Dan and Hansen, Jonas Meinertz and Winther, Ole},
  year = {2017},
  month = sep,
  number = {arXiv:1709.03933},
  eprint = {1709.03933},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We present hash embeddings, an efficient method for representing words in a continuous vector form. A hash embedding may be seen as an interpolation between a standard word embedding and a word embedding created using a random hash function (the hashing trick). In hash embeddings each token is represented by \$k\$ \$d\$-dimensional embeddings vectors and one \$k\$ dimensional weight vector. The final \$d\$ dimensional representation of the token is the product of the two. Rather than fitting the embedding vectors for each token these are selected by the hashing trick from a shared pool of \$B\$ embedding vectors. Our experiments show that hash embeddings can easily deal with huge vocabularies consisting of millions of tokens. When using a hash embedding there is no need to create a dictionary before training nor to perform any kind of vocabulary pruning after training. We show that models trained using hash embeddings exhibit at least the same level of performance as models trained using regular embeddings across a wide range of tasks. Furthermore, the number of parameters needed by such an embedding is only a fraction of what is required by a regular embedding. Since standard embeddings and embeddings constructed using the hashing trick are actually just special cases of a hash embedding, hash embeddings can be considered an extension and improvement over the existing regular embedding types.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/patrickhaller/Zotero/storage/7JILHT8B/Svenstrup et al. - 2017 - Hash Embeddings for Efficient Word Representations.pdf;/Users/patrickhaller/Zotero/storage/4RKXQIV4/1709.html}
}A
